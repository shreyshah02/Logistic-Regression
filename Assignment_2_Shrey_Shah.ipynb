{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the required libraries. Downloading the MNIST dataset from the keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training and testing data for the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the amount of training and testing data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains:  60000\n",
      "test set:  10000\n"
     ]
    }
   ],
   "source": [
    "print('The training set contains: ', len(x_train))\n",
    "print('test set: ', len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the shape of the training data and training label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the required preprocessing on the training and testing data. Reshaping the training data and testing data such that all the images are flattened into column and each column of the dataset X represents one training example. Additionally, normalizing the training and testing data by dividing each pixel value in the dataset by 255.  Reshaping the training and testing label to a row vector. The last digit of my student ID = 0, thus modifying the labels of the training and testing data to contain value 1 for an image containing the number 0 and a value of 0 otherwise. Additionally, printing the shapes of the training and testing data and labels to confirm correct preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(784, 10000)\n",
      "(1, 60000)\n",
      "(1, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Performing the preprocessing steps on the data set\n",
    "# Reshaping the training and test set such that each column represents an image\n",
    "# Reshaping the training and test labels into row vectors\n",
    "x_train = np.reshape(x_train, (60000, -1))\n",
    "x_train = x_train.T\n",
    "y_train = np.reshape(y_train, (1, -1))\n",
    "x_test = np.reshape(x_test, (10000, -1))\n",
    "x_test = x_test.T\n",
    "y_test = np.reshape(y_test, (1, -1))\n",
    "# Normalizing the data in training and test sets\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "# Updating the labels in the training and test set\n",
    "# Last digit of my student id = 0\n",
    "# Thus, the label of an image of number 0 should be 1 and 0 otherwise\n",
    "# This is achieved by creating masks for the training and testing labels such that the masks have True for indices\n",
    "# which correspond to an image of number 0\n",
    "id_mask_train = y_train == 0\n",
    "id_mask_test = y_test == 0\n",
    "y_train[id_mask_train] = 1\n",
    "y_train[np.logical_not(id_mask_train)] = 0\n",
    "y_test[id_mask_test] = 1\n",
    "y_test[np.logical_not(id_mask_test)] = 0\n",
    "# Checking the shape of the training and test sets\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to computing the sigmoid for an input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_sigmoid(z):\n",
    "    activ = 1/(1 + np.exp(-z))\n",
    "    return activ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for performing the Logistic Regression. The constructor method (__init__) loads and splits the training data into training + validation dataset. Additionally, it initializes the weights and the bias with random values. \n",
    "\n",
    "The method train_model, trains the model on the training set, starting with random weights and bias and performing the gradient descent for each value of alpha in the list provided during class initialization. From these the method tests each trained model with different alpha value by testing it on the validation set. The value of alpha which provides the least cost on the validation set is selected and used for training the model starting with random weights and bias and performing the gradeint descent algorithm. The number of epochs (iterations for the gradient descent algorithm) was set to 6000, after trying the values 1000, 2000, 3000, 5000, 6000, and 7000. The accuracy for 6000 epochs vs 7000 epochs was virtually identical and thus I used 6000 epochs.\n",
    "\n",
    "The method test_model, tests the trained model in the training step on the untouched test dataset, thus providing the performance of the trained model on previoulsy unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisticRegression :\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, alpha):\n",
    "        self.x_train = x_train[:, :48000]\n",
    "        self.y_train = y_train[:, :48000]\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.alpha = alpha\n",
    "        self.x_val = x_train[:, 48000:]\n",
    "        self.y_val = y_train[:, 48000:]\n",
    "        self.w = np.random.random((self.x_train.shape[0],1))\n",
    "        self.b = np.random.random()\n",
    "        self.m = self.x_train.shape[1]\n",
    "        \n",
    "    def train_model(self):\n",
    "        # Checking to see if a single value of alpha is provided or if multiple values are provided. \n",
    "        # If multiple values are provided then the best learning rate is selected by cross validation\n",
    "        if len(self.alpha)>1:\n",
    "            # To hold the cost of models learned by various alpha values\n",
    "            cost_val = []\n",
    "            # Looping over all the learning rates to determine the best one\n",
    "            for j in self.alpha:\n",
    "                print('Cross Validation for alpha = ', j)  \n",
    "                # Performing gradient descent to update the weights and bias of the model. The number of epochs is set  to 6000\n",
    "                # after experimenting with different values including 1000, 2000, 3000, 5000, 5000, 7000.\n",
    "                for i in range(6000):\n",
    "                    # Computing the affine transformation\n",
    "                    Z = self.w.T.dot(self.x_train) + self.b\n",
    "                    # Computing the sigmoid activation of Z\n",
    "                    A = activation_sigmoid(Z)\n",
    "                    \n",
    "                    # Computing the vectorized cost function\n",
    "                    logA = np.log(A)\n",
    "                    log1_A = np.log(1 - A)\n",
    "                    cost = - np.sum(self.y_train * logA + (1 - self.y_train) * log1_A) / self.m\n",
    "\n",
    "                    # Computing the gradients \n",
    "                    dZ = A - self.y_train\n",
    "                    dw = self.x_train.dot(dZ.T) / self.m\n",
    "                    db = np.sum(dZ)/self.m\n",
    "\n",
    "                    # Updating the weights and the bias\n",
    "                    self.w -= j * dw\n",
    "                    self.b -= j * db\n",
    "\n",
    "                # Computing the cost for the images in the validation set\n",
    "                # Computing the affine transformation\n",
    "                zval = self.w.T.dot(self.x_val) + self.b\n",
    "                # Computing the sigmoid activation value\n",
    "                Aval = activation_sigmoid(zval)\n",
    "                logAval = np.log(Aval)\n",
    "                log1_Aval = np.log(1 - Aval)\n",
    "                # Determining the cost on the validation set for the current learned model\n",
    "                cval = -np.sum(self.y_val * logAval + (1 - self.y_val)* log1_Aval)/self.x_val.shape[1]\n",
    "                # Appending the cost to the list containing cost for different alpha values on the validation set\n",
    "                cost_val.append(cval)\n",
    "                print('Cost for alpha = ', j, '= ', cval)\n",
    "                # Initializing weights randomly for the training with the next alpha value\n",
    "                self.w = np.random.random((self.x_train.shape[0],1))\n",
    "                self.b = np.random.random()\n",
    "\n",
    "            # Selecting the alpha value with the minimum cost on the validation set\n",
    "            alpha = self.alpha[cost_val.index(min(cost_val))]\n",
    "        else:\n",
    "            alpha = self.alpha[0]\n",
    "        print('The best learning rate = ', alpha)\n",
    "        print('Training the model using alpha = ' ,alpha)\n",
    "        self.w = np.random.random((self.x_train.shape[0],1))\n",
    "        self.b = np.random.random()\n",
    "        # Training the model using gradient descent with 6000 iterations\n",
    "        for i in range(6000):\n",
    "            Z = self.w.T.dot(self.x_train) + self.b\n",
    "            A = activation_sigmoid(Z)\n",
    "\n",
    "            logA = np.log(A)\n",
    "            log1_A = np.log(1 - A)\n",
    "            cost = - np.sum(self.y_train * logA + (1 - self.y_train) * log1_A) / self.m\n",
    "\n",
    "            dZ = A - self.y_train\n",
    "            dw = self.x_train.dot(dZ.T) / self.m\n",
    "            db = np.sum(dZ)/self.m\n",
    "\n",
    "            self.w -= alpha * dw\n",
    "            self.b -= alpha * db\n",
    "            \n",
    "            # Ploting the learning curve - the cost vs the iterations\n",
    "            plt.plot(i, cost, 'bo')\n",
    "            plt.xlabel('Iterations for Gradient Descent')\n",
    "            plt.ylabel('Cost function')\n",
    "            plt.title('Learning Curve')\n",
    "        \n",
    "        print('The training error = ', cost)\n",
    "        # Rounding the prediction values to the nearest integer\n",
    "        Apred = np.around(A)\n",
    "        # Computing the accuracy on the training dataset of the trained model\n",
    "        accuracy = np.mean(Apred == self.y_train)\n",
    "        print('The Training accuracy = ', accuracy)\n",
    "        return self.w, self.b\n",
    "    \n",
    "    def test_model(self):\n",
    "        # Performing the prediction on the test dataset using the trained model\n",
    "        ztest = self.w.T.dot(self.x_test) + self.b\n",
    "        Atest = activation_sigmoid(ztest)\n",
    "        \n",
    "        # Computing the cost on the test dataset\n",
    "        logA_test = np.log(Atest)\n",
    "        log1_Atest = np.log(1 - Atest)\n",
    "        cost_test = - np.sum(self.y_test * logA_test + (1 - self.y_test)* log1_Atest)/self.x_test.shape[1]\n",
    "        print('The testing error  = ', cost_test)\n",
    "        # Rounding the prediction values to the nearest integer\n",
    "        Apred = np.around(Atest)\n",
    "        # Computing the accuracy on the test dataset of the trained model\n",
    "        accuracy = np.mean(Apred == self.y_test)\n",
    "        print('Testing accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an object of the class for performing the logistic regression on the MNIST dataset. The various values of the alpha used for checking 5, 2, 1, 0.5, 0.2, 0.1. When the class method train_data is called, it determines the best learning rate from all provided as parameter during class initialization. Then using the best learning rate (which provides the minimum cost), the model is trained and the closed is plotted vs the iterations (epochs) of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Mode\n",
      "Cross Validation for alpha =  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\shrey\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for alpha =  5 =  0.025280193789209208\n",
      "Cross Validation for alpha =  2\n",
      "Cost for alpha =  2 =  0.024563777874491228\n",
      "Cross Validation for alpha =  1\n",
      "Cost for alpha =  1 =  0.025258516704746853\n",
      "Cross Validation for alpha =  0.5\n",
      "Cost for alpha =  0.5 =  0.026332561386352385\n",
      "The best learning rate =  2\n",
      "Training the model using alpha =  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrey\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:75: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\shrey\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:76: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error =  0.022221658814956618\n",
      "The Training accuracy =  0.9934375\n",
      "\n",
      "Testing Mode\n",
      "The testing error  =  0.025043834704278557\n",
      "Testing accuracy =  0.9918\n",
      "The Learning Curve is given as below:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = [5, 2, 1, 0.5]\n",
    "#alpha = [2]\n",
    "ID_notID = logisticRegression(x_train, y_train, x_test, y_test, alpha)\n",
    "print('\\nTraining Mode')\n",
    "w, b = ID_notID.train_model()\n",
    "print('\\nTesting Mode')\n",
    "ID_notID.test_model()\n",
    "print('The Learning Curve is given as below:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the trained model on random test images, one which is not 0 and another one which is a 0; and returning the prediction without rounding it. As it can be seen, for the ground truth value = 0, the prediction is extremely small (5 x 10^-8).\n",
    "\n",
    "Similarly, for the ground truth value = 1, the prediction = 0.999, which is very close to 1. Thus, the model is working well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :  [5.24796005e-08]\n",
      "Ground Truth:  0\n"
     ]
    }
   ],
   "source": [
    "Z_test_78 = w.T.dot(x_test[:, 78]) + b\n",
    "A_test_78 = activation_sigmoid(Z_test_78)\n",
    "print(\"Prediction : \", A_test_78)\n",
    "print('Ground Truth: ', y_test[0, 78])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth:  1\n",
      "prediction:  [0.99904722]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x218a94be828>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Ground Truth: ',y_test[0,3])\n",
    "z_test_3 = w.T.dot(x_test[:,3]) + b\n",
    "A = activation_sigmoid(z_test_3)\n",
    "print('prediction: ', A)\n",
    "plt.imshow(np.reshape(x_test[:, 3], (28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
